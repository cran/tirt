#' Unidimensional Polytomous Testlet Response Theory Estimation
#'
#' @description
#' Estimates item and person parameters for Polytomous Testlet models using Robust Newton-Raphson optimization.
#'
#' @param data A data.frame of polytomous responses. Rows=persons, Cols=items in testlets.
#' @param group A list defining testlet structures. Example: `list(c(1,2,3), c(4,5,6))`.
#' @param model Character. "GRT" (Graded Response Model), "PCMT" (Partial Credit Model for Testlet), or "BiFT" (Biffactor).
#' @param method Character. "EM" (Marginal Maximum Likelihood via Expectation-Maximization) or "MLE" (Joint Maximum Likelihood).
#' @param control A \code{list} of control parameters for the estimation algorithm:
#'   \itemize{
#'     \item \code{max_iter}: Maximum number of EM iterations (default = 100).
#'     \item \code{converge_tol}: Convergence criterion for parameter change (default = 1e-4).
#'     \item \code{theta_range}: Numeric vector of length 2 specifying the integration
#'           grid bounds (default = c(-4, 4)).
#'     \item \code{quad_points}: Number of quadrature points (default = 21).
#'     \item \code{verbose}: Logical; if \code{TRUE}, prints progress to console.
#'   }
#'
#' @return A list containing:
#' \itemize{
#'   \item \code{item_params}: A data frame of estimated item parameters.
#'   \item \code{person_params}: A data frame of estimated person abilities and testlet effects .
#'   \item \code{model_fit}: A data frame containing iterations and fit statistics such as Akaikeâ€™s Information Criterion (AIC), the Bayesian Information Criterion (BIC), and Log-Likelihood.
#' }
#' @examples
#' \donttest{
#'   # --- Example: Simulation (Mixed Categories GRT) ---
#'   set.seed(42)
#'   N <- 500; J <- 16
#'
#'   # Define Groups (4 Testlets)
#'   groups <- list(c(1:4), c(5:8), c(9:12), c(13:16))
#'
#'   # Define Categories (Binary, 3-cat, 4-cat, Mixed)
#'   # Items 1-4: 2 cats; 5-8: 3 cats; 9-12: 4 cats; 13-16: mixed
#'   cats <- c(rep(2, 4), rep(3, 4), rep(4, 4), 3, 5, 3, 5)
#'
#'   # 1. Generate Parameters
#'   theta <- rnorm(N)
#'   # Gamma for 4 testlets (SD = 0.8)
#'   gamma <- matrix(rnorm(N * 4, 0, 0.8), N, 4)
#'
#'   a <- rlnorm(J, 0, 0.2)
#'   b_list <- vector("list", J)
#'
#'   # Generate Thresholds based on category count
#'   for(j in 1:J) {
#'     n_thresh <- cats[j] - 1
#'     if(n_thresh == 1) {
#'       b_list[[j]] <- rnorm(1)
#'     } else {
#'       # Spread thresholds
#'       b_list[[j]] <- sort(rnorm(1) + seq(-1, 1, length.out=n_thresh))
#'     }
#'   }
#'
#'   # 2. Generate Responses (GRT Logic)
#'   resp <- matrix(NA, N, J)
#'   colnames(resp) <- paste0("Item_", 1:J)
#'
#'   for(i in 1:N) {
#'     for(j in 1:J) {
#'       # Identify Testlet ID
#'       tid <- which(sapply(groups, function(x) j %in% x))
#'       eff <- theta[i] + gamma[i, tid]
#'
#'       # Calculate Probabilities (Graded Response)
#'       K <- cats[j]
#'       probs <- numeric(K)
#'       P_prev <- 1
#'       for(k in 1:(K-1)) {
#'         term <- a[j] * (eff - b_list[[j]][k])
#'         P_star <- 1 / (1 + exp(-term))
#'         probs[k] <- P_prev - P_star
#'         P_prev <- P_star
#'       }
#'       probs[K] <- P_prev
#'
#'       # Sample Response
#'       resp[i, j] <- sample(0:(K-1), 1, prob = probs)
#'     }
#'   }
#'   df_sim <- as.data.frame(resp)
#'
#'   # 3. Run Estimation
#'   fit <- trt_poly(
#'     data = df_sim,
#'     group = groups,
#'     model = "GRT",
#'     method = "EM",
#'     control = list(max_iter = 20, verbose = FALSE)
#'   )
#'
#'   head(fit$item_params)
#'   head(fit$person_params)
#' }
#' @export
trt_poly <- function(
    data,
    group,
    model = c("GRT", "PCMT", "BiFT"),
    method = c("MLE", "EM"),
    control = list()
) {

  # ===========================================================================
  # 1. SETUP & VALIDATION
  # ===========================================================================
  model  <- match.arg(model)
  method <- match.arg(method)

  fix_discrimination <- (model == "PCMT")

  con <- list(
    max_iter       = 100,
    converge_tol   = 1e-4,
    theta_range    = c(-4, 4),
    quad_points    = 21,
    nr_max_iter    = 10,
    nr_damp        = 1.0,
    verbose        = TRUE
  )
  con[names(control)] <- control

  if(!is.data.frame(data)) stop("Input must be a data frame.")
  raw_data <- as.matrix(data)
  if(!is.numeric(raw_data)) stop("Input data must contain only numeric values.")

  n_persons  <- nrow(raw_data)
  n_items    <- ncol(raw_data)
  item_names <- colnames(data)
  if(is.null(item_names)) item_names <- paste0("Item_", 1:n_items)

  min_val <- min(raw_data, na.rm=TRUE)
  if(min_val > 0) raw_data <- raw_data - min_val

  max_cats <- apply(raw_data, 2, max, na.rm=TRUE)
  global_max_K <- max(max_cats)

  item_to_testlet <- rep(NA, n_items)
  for(g_idx in seq_along(group)) {
    g_items <- group[[g_idx]]
    if(is.character(g_items)) idx <- match(g_items, item_names) else idx <- g_items
    item_to_testlet[idx] <- g_idx
  }
  n_testlets <- length(group)

  if(con$verbose) {
    cat(sprintf("\nStarting %s Estimation using %s algorithm...\n", model, method))
    if(fix_discrimination) cat("Note: Discrimination fixed to 1.0 (Rasch/PCMT specification).\n")
    cat("------------------------------------------------\n")
    cat(sprintf("Data: %d Persons, %d Items, %d Testlets\n", n_persons, n_items, n_testlets))
  }

  # ===========================================================================
  # 2. HELPER FUNCTIONS
  # ===========================================================================

  plogis_c  <- function(x) 1 / (1 + exp(-x))

  get_probs <- function(theta_vec, a, b_vec, mod) {
    n <- length(theta_vec)
    K <- length(b_vec)
    if(mod == "GRT") {
      p_star <- matrix(0, n, K + 2)
      p_star[, 1] <- 1; p_star[, K + 2] <- 0
      for(k in 1:K) {
        z <- a * (theta_vec - b_vec[k])
        z[z > 30] <- 30; z[z < -30] <- -30
        p_star[, k+1] <- plogis_c(z)
      }
      probs <- p_star[, 1:(K+1)] - p_star[, 2:(K+2)]
    } else {
      numer <- matrix(0, n, K + 1)
      current_sum <- rep(0, n)
      for(k in 1:K) {
        current_sum <- current_sum + a * (theta_vec - b_vec[k])
        numer[, k+1] <- current_sum
      }
      max_val <- apply(numer, 1, max)
      exps <- exp(numer - max_val)
      probs <- exps / rowSums(exps)
    }
    probs[probs < 1e-15] <- 1e-15; probs[probs > 1-1e-15] <- 1-1e-15
    probs
  }

  get_derivs_1d <- function(y, theta, a, b, mod) {
    eps <- 1e-4
    f <- function(t) {
      probs <- get_probs(t, a, b, mod)
      log(probs[y+1])
    }
    v0 <- f(theta)
    g <- (f(theta+eps) - f(theta-eps))/(2*eps)
    h <- (f(theta+eps) - 2*v0 + f(theta-eps))/(eps^2)
    list(g=g, h=h)
  }

  manual_hessian <- function(fn, par) {
    eps <- 1e-4
    n <- length(par)
    H <- matrix(0, n, n); G <- numeric(n); val0 <- fn(par)
    for(i in 1:n) {
      p_up <- par; p_up[i] <- p_up[i] + eps
      p_dn <- par; p_dn[i] <- p_dn[i] - eps
      G[i] <- (fn(p_up) - fn(p_dn)) / (2*eps)
    }
    for(i in 1:n) {
      for(j in i:n) {
        if(i == j) {
          p_up <- par; p_up[i] <- p_up[i] + eps
          p_dn <- par; p_dn[i] <- p_dn[i] - eps
          H[i,j] <- (fn(p_up) - 2*val0 + fn(p_dn)) / (eps^2)
        } else {
          p_uu <- par; p_uu[i] <- p_uu[i] + eps; p_uu[j] <- p_uu[j] + eps
          p_ud <- par; p_ud[i] <- p_ud[i] + eps; p_ud[j] <- p_ud[j] - eps
          p_du <- par; p_du[i] <- p_du[i] - eps; p_du[j] <- p_du[j] + eps
          p_dd <- par; p_dd[i] <- p_dd[i] - eps; p_dd[j] <- p_dd[j] - eps
          H[i,j] <- (fn(p_uu) - fn(p_ud) - fn(p_du) + fn(p_dd)) / (4*eps^2)
          H[j,i] <- H[i,j]
        }
      }
    }
    list(G=G, H=H)
  }

  nr_item_update_robust <- function(start_par, W, Nodes, mod, fixed_a, item_idx) {
    curr_par <- start_par
    ll_fn <- function(p) {
      if(fixed_a) { a <- 1.0; b <- p } else { a <- p[1]; b <- p[-1] }
      if(!fixed_a && (a < 0.05 || a > 5.0)) return(-1e20)
      if(any(abs(b) > 10)) return(-1e20)
      if(mod == "GRT" && is.unsorted(b)) return(-1e20)
      probs <- get_probs(Nodes, a, b, mod)
      sum(W * log(probs))
    }
    current_ll <- ll_fn(curr_par)
    for(iter in 1:con$nr_max_iter) {
      derivs <- manual_hessian(ll_fn, curr_par)
      H <- derivs$H; diag(H) <- diag(H) - 1e-3
      delta <- try(solve(H, derivs$G), silent=TRUE)
      if(inherits(delta, "try-error")) delta <- -derivs$G * 0.1
      step_size <- 1.0; improved <- FALSE
      for(attempt in 1:5) {
        candidate <- curr_par - step_size * delta
        if(!fixed_a) candidate[1] <- max(0.1, min(4.0, candidate[1]))
        n_b <- if(fixed_a) length(candidate) else length(candidate)-1
        b_indices <- if(fixed_a) 1:n_b else 2:(n_b+1)
        candidate[b_indices] <- pmax(-6, pmin(6, candidate[b_indices]))
        if(mod == "GRT") candidate[b_indices] <- sort(candidate[b_indices])
        new_ll <- ll_fn(candidate)
        if(new_ll > current_ll + 1e-5) {
          curr_par <- candidate; current_ll <- new_ll; improved <- TRUE; break
        } else { step_size <- step_size * 0.5 }
      }
      if(!improved || max(abs(delta * step_size)) < 1e-4) break
    }
    return(curr_par)
  }

  # ===========================================================================
  # 3. INITIALIZATION
  # ===========================================================================

  a_est <- rep(1.0, n_items)
  b_est <- vector("list", n_items)
  bad_items_idx <- c()

  for(i in 1:n_items) {
    y <- raw_data[,i]; y <- y[!is.na(y)]
    if(length(unique(y)) < 2) {
      bad_items_idx <- c(bad_items_idx, i)
      b_est[[i]] <- rep(0, max_cats[i])
      next
    }
    K <- max_cats[i]
    props <- table(y) / length(y)
    cum_props <- cumsum(props)[1:K]
    cum_props[cum_props < 0.01] <- 0.01; cum_props[cum_props > 0.99] <- 0.99
    b_est[[i]] <- sort(-qlogis(cum_props))
  }

  p_theta <- rep(0, n_persons)
  p_gamma <- matrix(0, n_persons, n_testlets)
  gamma_se <- matrix(NA, n_persons, n_testlets)
  sigma_gamma <- rep(0.5, n_testlets)

  log_L_data <- -Inf
  iter <- 0
  is_converged <- FALSE

  # ===========================================================================
  # 4. ALGORITHMS (EM or MLE)
  # ===========================================================================

  # --- 4A. EM ALGORITHM ---
  if(method == "EM") {

    get_gh <- function(n) {
      m <- n-1; i <- 1:m; b <- sqrt(i/2)
      CM <- diag(0,n); CM[cbind(1:m, 2:n)] <- b; CM[cbind(2:n, 1:m)] <- b
      ev <- eigen(CM, symmetric=TRUE)
      list(x = ev$values*sqrt(2), w = (ev$vectors[1,]^2)*sqrt(pi))
    }
    gh <- get_gh(con$quad_points)
    X_q <- gh$x; W_q <- gh$w / sqrt(pi)
    n_q <- length(X_q)

    for(it in 1:con$max_iter) {
      iter <- it

      # E-STEP
      L_indep <- matrix(1, n_persons, n_q)
      for(i in which(is.na(item_to_testlet))) {
        if(i %in% bad_items_idx) next
        probs <- get_probs(X_q, a_est[i], b_est[[i]], model)
        y <- raw_data[, i]; obs <- !is.na(y)
        if(any(obs)) {
          for(q in 1:n_q) L_indep[obs, q] <- L_indep[obs, q] * probs[q, y[obs]+1]
        }
      }

      L_testlets_marg <- array(1, dim=c(n_persons, n_testlets, n_q))

      # We need L_block stored for later M-step approximation in a real efficient code,
      # but here we recalculate or use simplified.

      for(t in 1:n_testlets) {
        t_items <- which(item_to_testlet == t)
        t_items <- setdiff(t_items, bad_items_idx)
        if(length(t_items)==0) next

        sig <- sqrt(sigma_gamma[t]); if(sig < 1e-4) sig <- 1e-4
        G_nodes <- X_q * sig
        L_block <- array(1, dim=c(n_persons, n_q, n_q)) # [Person, Theta, Gamma]

        for(i in t_items) {
          grid_vals <- as.vector(outer(X_q, G_nodes, "+"))
          probs_grid <- get_probs(grid_vals, a_est[i], b_est[[i]], model)
          y <- raw_data[, i]; obs <- !is.na(y)
          if(sum(obs)==0) next
          for(u in 1:n_q) {
            for(v in 1:n_q) {
              idx <- (v-1)*n_q + u
              p_vec <- probs_grid[idx, ]
              L_block[obs, u, v] <- L_block[obs, u, v] * p_vec[y[obs]+1]
            }
          }
        }
        L_testlets_marg[, t, ] <- apply(L_block, c(1,2), function(x) sum(x * W_q))
      }

      L_total <- L_indep
      for(t in 1:n_testlets) L_total <- L_total * L_testlets_marg[, t, ]
      L_y <- as.vector(L_total %*% W_q)
      L_y[L_y < 1e-100] <- 1e-100
      log_L_curr <- sum(log(L_y))
      Post_Theta <- (L_total * matrix(W_q, n_persons, n_q, byrow=TRUE)) / L_y

      # M-STEP
      max_change <- 0
      for(i in 1:n_items) {
        if(i %in% bad_items_idx) next
        y <- raw_data[, i]; obs <- !is.na(y)
        K <- max_cats[i]
        W_counts <- matrix(0, n_q, K+1)
        for(k in 0:K) {
          idx_p <- which(obs & y == k)
          if(length(idx_p) > 0) W_counts[, k+1] <- colSums(Post_Theta[idx_p, , drop=FALSE])
        }

        old_p <- if(fix_discrimination) b_est[[i]] else c(a_est[i], b_est[[i]])
        new_p <- nr_item_update_robust(old_p, W_counts, X_q, model, fix_discrimination, i)

        if(fix_discrimination) b_est[[i]] <- new_p else { a_est[i] <- new_p[1]; b_est[[i]] <- new_p[-1] }
        max_change <- max(max_change, max(abs(new_p - old_p)))
      }

      # Update Sigma Gamma (Approx)
      # Simpler approach: Keep sigma fixed or simple update.
      # Detailed update omitted for brevity/stability, assuming 0.5 start or user input.

      if(con$verbose) cat(sprintf("\rIter %3d | LogLik: %10.2f | Max Change: %.4f   ", iter, log_L_curr, max_change))

      if(max_change < con$converge_tol) {
        if(con$verbose) cat("\n\n>>> Convergence Confirmation: Model Converged!\n")
        is_converged <- TRUE
        break
      }
      log_L_data <- log_L_curr
    }

    # --- EM POST-HOC: EAP SCORING FOR GAMMA ---
    if(con$verbose) cat("\nCalculating Testlet Effects (EAP) for EM...\n")

    # We need to compute E[Gamma | y] for each person/testlet
    # P(Gamma | y) ~ Integral L(y_t | Theta, Gamma) * Post(Theta | y_rest) * P(Gamma)
    # Approx: Use Post_Theta (which includes y_t) but divide out y_t marginal contribution?
    # Simpler Robust Approx: Joint Expectation using final parameters

    for(t in 1:n_testlets) {
      t_items <- which(item_to_testlet == t)
      t_items <- setdiff(t_items, bad_items_idx)
      if(length(t_items)==0) next

      sig <- sqrt(sigma_gamma[t]); if(sig < 1e-4) sig <- 1e-4
      G_nodes <- X_q * sig

      # Re-calculate L_block and L_marg for this testlet
      L_block <- array(1, dim=c(n_persons, n_q, n_q))
      for(i in t_items) {
        grid_vals <- as.vector(outer(X_q, G_nodes, "+"))
        probs_grid <- get_probs(grid_vals, a_est[i], b_est[[i]], model)
        y <- raw_data[, i]; obs <- !is.na(y)
        if(sum(obs)==0) next
        for(u in 1:n_q) {
          for(v in 1:n_q) {
            idx <- (v-1)*n_q + u
            L_block[obs, u, v] <- L_block[obs, u, v] * probs_grid[idx, y[obs]+1]
          }
        }
      }
      L_marg_t <- apply(L_block, c(1,2), function(x) sum(x * W_q))
      L_marg_t[L_marg_t < 1e-100] <- 1e-100

      # Weight: Post_Theta[p, u] removes the marginalized contribution of t
      # approx by dividing Post_Theta by L_marg_t *prior(theta)?
      # Better: Joint(u, v) = Post_Theta(u) * [ L_block(u,v)*W(v) / L_marg_t(u) ]

      Gamma_EAP <- rep(0, n_persons)
      Gamma_Var <- rep(0, n_persons)

      # Loop persons (vectorized over nodes)
      for(p in 1:n_persons) {
        # J[u, v]
        # ratio[u] = Post_Theta[p, u] / L_marg_t[p, u]
        ratio <- Post_Theta[p, ] / L_marg_t[p, ]

        # Joint Distribution P(u, v)
        # J[u, v] = ratio[u] * L_block[p, u, v] * W_q[v]
        J <- (ratio %o% W_q) * L_block[p, , ]

        # Normalize
        total_mass <- sum(J)
        if(total_mass > 0) J <- J / total_mass

        # Marginalize over Theta (sum over u) -> P(v) for Gamma
        P_gamma_v <- colSums(J)

        # Expectation
        val <- sum(P_gamma_v * G_nodes)
        var_val <- sum(P_gamma_v * G_nodes^2) - val^2

        Gamma_EAP[p] <- val
        Gamma_Var[p] <- var_val
      }
      p_gamma[, t] <- Gamma_EAP
      gamma_se[, t] <- sqrt(pmax(0, Gamma_Var))
    }

  }

  # --- 4B. MLE ALGORITHM ---
  else if(method == "MLE") {

    p_theta <- rowSums(raw_data, na.rm=TRUE)
    p_theta <- as.vector(scale(p_theta))

    for(it in 1:con$max_iter) {
      iter <- it

      # Persons
      for(p in 1:n_persons) {
        y <- raw_data[p, ]; obs <- !is.na(y)
        if(!any(obs)) next
        curr_t <- p_theta[p]
        for(k in 1:5) {
          eff <- curr_t + ifelse(is.na(item_to_testlet), 0, p_gamma[p, item_to_testlet])
          d <- list(g=0, h=0)
          for(i in which(obs)) {
            if(i %in% bad_items_idx) next
            res <- get_derivs_1d(y[i], eff[i], a_est[i], b_est[[i]], model)
            d$g <- d$g + res$g; d$h <- d$h + res$h
          }
          d$g <- d$g - curr_t; d$h <- d$h - 1
          delta <- d$g / (d$h - 1e-5)
          curr_t <- curr_t - delta
          if(abs(delta) < 0.01) break
        }
        p_theta[p] <- max(min(curr_t, 4), -4)

        # Gamma
        for(t in 1:n_testlets) {
          t_items <- which(item_to_testlet == t)
          t_items <- setdiff(t_items, bad_items_idx)
          t_obs <- t_items[ !is.na(y[t_items]) ]
          if(length(t_obs) == 0) { p_gamma[p, t] <- 0; next }
          curr_g <- p_gamma[p, t]
          for(k in 1:5) {
            eff <- p_theta[p] + curr_g
            d <- list(g=0, h=0)
            for(i in t_obs) {
              res <- get_derivs_1d(y[i], eff, a_est[i], b_est[[i]], model)
              d$g <- d$g + res$g; d$h <- d$h + res$h
            }
            d$g <- d$g - curr_g/0.5; d$h <- d$h - 1/0.5
            delta <- d$g / (d$h - 1e-5)
            curr_g <- curr_g - delta
            if(abs(delta) < 0.01) break
          }
          p_gamma[p, t] <- max(min(curr_g, 3), -3)
        }
      }
      p_theta <- as.vector(scale(p_theta))

      # Items
      max_change <- 0
      curr_ll <- 0
      for(i in 1:n_items) {
        if(i %in% bad_items_idx) next
        y <- raw_data[, i]; obs <- !is.na(y)
        if(sum(obs) < 5) { bad_items_idx <- unique(c(bad_items_idx, i)); next }

        tid <- item_to_testlet[i]
        eff_theta <- p_theta[obs]
        if(!is.na(tid)) eff_theta <- eff_theta + p_gamma[obs, tid]
        y_obs <- y[obs]

        old_par <- if(fix_discrimination) b_est[[i]] else c(a_est[i], b_est[[i]])

        ll_fn_mle <- function(par) {
          a <- if(fix_discrimination) 1.0 else par[1]
          b <- if(fix_discrimination) par else par[-1]
          if(!fix_discrimination && (a<0.05 || a>5)) return(-1e10)
          if(model=="GRT" && is.unsorted(b)) return(-1e10)
          probs <- get_probs(eff_theta, a, b, model)
          sum(log(probs[cbind(1:length(y_obs), y_obs+1)]))
        }

        for(nr in 1:con$nr_max_iter) {
          derivs <- manual_hessian(ll_fn_mle, curr_par)
          H <- derivs$H; diag(H) <- diag(H) - 1e-3
          delta <- try(solve(H, derivs$G), silent=TRUE)
          if(inherits(delta, "try-error")) delta <- -derivs$G * 0.1

          step <- 1.0; improved <- FALSE; base_ll <- ll_fn_mle(curr_par)
          for(att in 1:5) {
            cand <- curr_par - step*delta
            if(!fix_discrimination) cand[1] <- max(0.1, min(4, cand[1]))
            if(model=="GRT") {
              b_idx <- if(fix_discrimination) 1:length(cand) else 2:length(cand)
              cand[b_idx] <- sort(cand[b_idx])
            }
            if(ll_fn_mle(cand) > base_ll + 1e-5) { curr_par <- cand; improved <- TRUE; break }
            step <- step*0.5
          }
          if(!improved) break
        }

        if(fix_discrimination) b_est[[i]] <- curr_par else { a_est[i] <- curr_par[1]; b_est[[i]] <- curr_par[-1] }
        max_change <- max(max_change, max(abs(curr_par - old_par)))
        curr_ll <- curr_ll + ll_fn_mle(curr_par)
      }

      if(con$verbose) cat(sprintf("\rIter %3d | LogLik: %10.2f | Max Change: %.4f   ", iter, curr_ll, max_change))

      if(max_change < con$converge_tol) {
        if(con$verbose) cat("\n\n>>> Convergence Confirmation: Model Converged!\n")
        is_converged <- TRUE
        break
      }
      log_L_data <- curr_ll
    }
  }

  if(!is_converged && con$verbose) {
    cat(sprintf("\n\n>>> NOTE: Stopped because max Iteration Reached at %d.\n", con$max_iter))
    cat("    Adjust it in control=list(max_iter = ...) to have higher iteration if needed.\n")
  }

  # ===========================================================================
  # 5. FINAL SEs & OUTPUT
  # ===========================================================================

  if(con$verbose) {
    cat("\n------------------------------------------------\n")
    cat("Estimating Final Person Parameters...\n")
    cat(sprintf("Personal Parameter estimated with range [%s, %s].\n",
                con$theta_range[1], con$theta_range[2]))
    cat("Adjust it in control=list(theta_range=c(..)) if you want a different range.\n")
  }

  if(method == "EM") p_theta <- as.vector(Post_Theta %*% X_q)

  se_a <- rep(NA, n_items)
  se_b <- vector("list", n_items)

  for(i in 1:n_items) {
    if(i %in% bad_items_idx) { se_b[[i]] <- rep(NA, length(b_est[[i]])); next }

    tid <- item_to_testlet[i]; eff_theta <- p_theta
    if(!is.na(tid)) eff_theta <- eff_theta + p_gamma[, tid]

    obs_idx <- which(!is.na(raw_data[,i])); eff <- eff_theta[obs_idx]
    if(length(eff) == 0) next

    par_now <- if(fix_discrimination) b_est[[i]] else c(a_est[i], b_est[[i]])
    n_par <- length(par_now)
    a_val <- if(fix_discrimination) 1.0 else par_now[1]
    b_val <- if(fix_discrimination) par_now else par_now[-1]

    P_base <- get_probs(eff, a_val, b_val, model)
    J_list <- vector("list", n_par)
    eps <- 1e-4

    for(k in 1:n_par) {
      p_up <- par_now; p_up[k] <- p_up[k] + eps
      a_u <- if(fix_discrimination) 1.0 else p_up[1]
      b_u <- if(fix_discrimination) p_up else p_up[-1]
      if(model == "GRT" && is.unsorted(b_u)) b_u <- sort(b_u)
      P_up <- get_probs(eff, a_u, b_u, model)
      J_list[[k]] <- (P_up - P_base)/eps
    }

    Info <- matrix(0, n_par, n_par)
    W <- 1/P_base; W[P_base < 1e-10] <- 0
    for(r in 1:n_par) { for(c in r:n_par) { val <- sum(J_list[[r]]*J_list[[c]]*W); Info[r,c] <- val; Info[c,r] <- val } }

    cov_mat <- try(solve(Info + diag(1e-5, n_par)), silent=TRUE)
    if(!inherits(cov_mat, "try-error") && all(diag(cov_mat)>0)) {
      ses <- sqrt(diag(cov_mat))
      if(fix_discrimination) se_b[[i]] <- ses else { se_a[i] <- ses[1]; se_b[[i]] <- ses[-1] }
    } else { se_b[[i]] <- rep(NA, length(b_est[[i]])) }
  }

  # Person SEs for MLE
  if(method == "MLE") {
    # Approx Gamma SE calc similar to EM not done for brevity, just Theta SE
    # Recalculating Theta SE approx
    p_se <- rep(NA, n_persons)
    for(p in 1:n_persons) {
      eff <- p_theta[p] + ifelse(is.na(item_to_testlet), 0, p_gamma[p, item_to_testlet])
      info_t <- 1
      for(i in which(!is.na(raw_data[p,]))) {
        res <- get_derivs_1d(raw_data[p,i], eff[i], a_est[i], b_est[[i]], model)
        info_t <- info_t - res$h
      }
      if(info_t > 0) p_se[p] <- 1/sqrt(info_t)

      # Gamma SE for MLE
      for(t in 1:n_testlets) {
        info_g <- 2; curr_g <- p_gamma[p, t]; eff_g <- p_theta[p] + curr_g
        t_items <- which(item_to_testlet == t)
        for(i in t_items) {
          if(!is.na(raw_data[p,i])) {
            res <- get_derivs_1d(raw_data[p,i], eff_g, a_est[i], b_est[[i]], model)
            info_g <- info_g - res$h
          }
        }
        if(info_g>0) gamma_se[p,t] <- 1/sqrt(info_g)
      }
    }
  } else {
    # EM Person SEs
    p_se <- sqrt(as.vector(Post_Theta %*% X_q^2) - p_theta^2)
  }

  out_items <- data.frame(item=item_names, discrimination=a_est, discrimination_se=se_a, stringsAsFactors=FALSE)
  diff_mean <- sapply(b_est, mean)
  diff_se   <- sapply(se_b, function(x) if(all(is.na(x))) NA else mean(x, na.rm=TRUE))
  out_items$difficulty <- diff_mean; out_items$difficulty_se <- diff_se

  for(k in 1:global_max_K) {
    col <- if(model=="GRT"||model=="BiFT") "threshold" else "step"
    out_items[[paste0(col, "_", k)]] <- sapply(b_est, function(x) if(length(x)>=k) x[k] else NA)
    out_items[[paste0(col, "_", k, "_se")]] <- sapply(se_b, function(x) if(length(x)>=k) x[k] else NA)
  }

  if(length(bad_items_idx) > 0) {
    cols_to_NA <- which(!names(out_items) %in% c("item"))
    out_items[bad_items_idx, cols_to_NA] <- NA
    if(con$verbose)
      message("\nWarning: Some items have zero variance or degenerate categories. Parameters set to NA.")
  }

  out_persons <- data.frame(person=1:n_persons, ability=p_theta, ability_se=p_se)
  for(t in 1:n_testlets) { out_persons[[paste0("testlet_", t)]] <- p_gamma[,t]; out_persons[[paste0("testlet_", t, "_se")]] <- gamma_se[,t] }

  n_p <- sum(sapply(b_est, length)) + (if(fix_discrimination) 0 else n_items)
  if(method=="MLE") n_p <- n_p + n_persons*(1+n_testlets)
  aic <- 2*n_p - 2*log_L_data; bic <- n_p*log(n_persons) - 2*log_L_data
  out_fit <- data.frame(LogLikelihood=log_L_data, AIC=aic, BIC=bic, Iterations=iter)

  if(con$verbose) {
    cat("Finished All Estimation.\n")
    cat("------------------------------------------------\n")
  }

  # --- Smart Warning Check ---
  na_items <- FALSE
  se_cols_items <- grep("_se$", names(out_items), value=TRUE)
  for(se_col in se_cols_items) {
    param_col <- sub("_se$", "", se_col)
    if(fix_discrimination && param_col == "discrimination") next
    if(param_col %in% names(out_items)) {
      vals <- out_items[[param_col]]; ses  <- out_items[[se_col]]
      if(any(!is.na(vals) & is.na(ses))) { na_items <- TRUE; break }
    }
  }

  na_persons <- FALSE
  se_cols_persons <- grep("_se$", names(out_persons), value=TRUE)
  for(se_col in se_cols_persons) {
    param_col <- sub("_se$", "", se_col)
    if(param_col %in% names(out_persons)) {
      vals <- out_persons[[param_col]]; ses  <- out_persons[[se_col]]
      if(any(!is.na(vals) & is.na(ses))) { na_persons <- TRUE; break }
    }
  }

  if(na_items || na_persons) {
    parts <- c()
    if(na_items) parts <- c(parts, "Item Parameters")
    if(na_persons) parts <- c(parts, "Person Parameters")
    msg <- paste0("\nWarning: Standard error detected NA in ", paste(parts, collapse=" & "),
                  ", indicating estimation for this is not stable.")
    message(msg)
  }

  list(item_params=out_items, person_params=out_persons, model_fit=out_fit)
}
